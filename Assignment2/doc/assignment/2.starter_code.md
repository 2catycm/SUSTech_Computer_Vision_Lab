**Using the starter code** (proj2.ipynb)

The top-level proj2.ipynb IPython notebook provided in the starter code
includes file handling, visualization, and evaluation functions for you
as well as calls to placeholder versions of the three functions listed
above. Running the starter code without modification will visualize
random interest points matched randomly on the particular Notre Dame
images shown at the top of this page. The correspondence will be
visualized with show_correspondence_circles() and
show_correspondence_lines() (you can comment one or both out if you
prefer).

For the Notre Dame image pair there is a ground truth evaluation in the
starter code as well. evaluate_correspondence() will classify each match
as correct or incorrect based on hand-provided matches (see
show_ground_truth_corr() for details). The starter code also contains
ground truth correspondences for two other image pairs (Mount Rushmore
and Episcopal Gaudi). You can test on those images by uncommenting the
appropriate lines in proj2.ipynb.

You can create additional ground truth matches with the
CorrespondenceAnnotator().collect_ground_truth_corr() found in
annotate_correspondences/collect_ground_truth_corr.py (but it\'s a
tedious process).

As you implement your feature matching pipeline, you should see your
performance according to evaluate_correspondence() increase. Hopefully
you find this useful, but don\'t overfit to the initial Notre Dame image
pair which is relatively easy. The baseline algorithm suggested here and
in the starter code will give you full credit and work fairly well on
these Notre Dame images, but additional image pairs provided in
extra_data.zip are more difficult. They might exhibit more viewpoint,
scale, and illumination variation. If you add enough you should be able
to match more difficult image pairs.